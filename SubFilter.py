import sys
import json
import argparse
from urllib.parse import urlparse, parse_qs

# Function to load results from tools based on the tool type
def load_tool_results(file_path, tool_type="subfinder"):
    """Load subdomain tool outputs based on the selected tool"""
    
    if tool_type == "amass":
        return load_amass_results(file_path)  # Process Amass results
    elif tool_type == "subfinder":
        return load_subfinder_results(file_path)  # Process Subfinder results
    elif tool_type == "sublist3r":
        return load_sublist3r_results(file_path)  # Process Sublist3r results
    elif tool_type == "aquatone":
        return load_aquatone_results(file_path)  # Process Aquatone results
    elif tool_type == "knockpy":
        return load_knockpy_results(file_path)  # Process Knockpy results
    else:
        raise ValueError(f"Unsupported tool type: {tool_type}")

def load_amass_results(file_path):
    """Load Amass results (JSON format)"""
    with open(file_path, 'r') as file:
        data = json.load(file)
        return [entry['name'] for entry in data['hostnames']]

def load_subfinder_results(file_path):
    """Load Subfinder results (TXT format)"""
    with open(file_path, 'r') as file:
        return [line.strip() for line in file.readlines()]

def load_sublist3r_results(file_path):
    """Load Sublist3r results (TXT format)"""
    with open(file_path, 'r') as file:
        return [line.strip() for line in file.readlines()]

def load_aquatone_results(file_path):
    """Load Aquatone results (HTML or CSV format)"""
    pass

def load_knockpy_results(file_path):
    """Load Knockpy results (TXT format)"""
    with open(file_path, 'r') as file:
        return [line.strip() for line in file.readlines()]

# Function to filter URLs based on the tool used
def filter_urls(urls, tool_type="subfinder"):
    """Filter URLs based on the tool used"""
    
    if tool_type in ["subfinder", "sublist3r", "knockpy"]:
        return filter_simple_urls(urls)  # If the tool generates a simple text file
    elif tool_type == "amass":
        return filter_amass_urls(urls)  # If the tool generates a JSON file
    elif tool_type == "aquatone":
        return filter_aquatone_urls(urls)  # If the tool generates HTML or CSV
    else:
        raise ValueError(f"Unsupported tool type for filtering: {tool_type}")

def filter_simple_urls(urls):
    """Filter simple (text) URLs"""
    seen = set()
    filtered = []
    
    for url in urls:
        parsed = urlparse(url)
        base_url = f"{parsed.scheme}://{parsed.netloc}{parsed.path}"
        params = parse_qs(parsed.query)
        
        # Convert parameters to tuple instead of list to avoid errors
        params_tuple = tuple((key, tuple(value)) for key, value in params.items())
        
        # Remove duplicates based on base_url + parameters
        if (base_url, frozenset(params_tuple)) not in seen:
            filtered.append(url)
            seen.add((base_url, frozenset(params_tuple)))
    
    return filtered

def filter_amass_urls(urls):
    """Filter URLs generated by Amass (JSON format)"""
    return set(urls)  # Simple filtering model

def filter_aquatone_urls(urls):
    """Filter URLs generated by Aquatone (HTML or CSV format)"""
    return set(urls)  # Simple filtering model

# Function to save filtered URLs
def save_filtered_urls(filtered_urls, output_file="filtered_urls.txt"):
    """Save filtered URLs to a text file."""
    with open(output_file, 'w') as file:
        for url in filtered_urls:
            file.write(url + "\n")
    print(f"Filtered URLs saved to {output_file}")

# Function to handle the command line interface
def main():
    parser = argparse.ArgumentParser(description="Filter subdomains from different tools.")
    parser.add_argument("input_file", help="Input file containing subdomains (from an external tool)")
    parser.add_argument("--tool", choices=["amass", "subfinder", "sublist3r", "aquatone", "knockpy"], required=True, help="Tool used to generate the subdomain list")
    parser.add_argument("--output", default="filtered_subdomains.txt", help="Output file to save filtered results")
    
    args = parser.parse_args()

    # Load results based on the tool type
    urls = load_tool_results(args.input_file, args.tool)

    # Filter the URLs
    filtered_urls = filter_urls(urls, args.tool)

    # Save the filtered URLs
    save_filtered_urls(filtered_urls, args.output)

if __name__ == "__main__":
    main()
